{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "\n",
    "# modules to handle data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create submission file with assigned predicted results from models\n",
    "def create_file_for_submission(filename, classifier_predictions):\n",
    "    kaggle_submission = pd.read_csv('./datasets/IEEEFraudDetection/sample_submission.csv', index_col='TransactionID')\n",
    "\n",
    "    # Probabilities of class 2 ('isFraud'=1) is in column 1 of the matrix\n",
    "    kaggle_submission['isFraud'] = classifier_predictions[:,1]\n",
    "    kaggle_submission.to_csv(filename)\n",
    "    \n",
    "def load_classifier_from_picklefile(filename):\n",
    "    infile = open(filename,'rb')\n",
    "    loaded_Classifier = pickle.load(infile)\n",
    "    infile.close()\n",
    "    return loaded_Classifier\n",
    "    \n",
    "def save_classifier_to_picklefile(filename, save_Classifier):\n",
    "    model_file = open(filename,'wb')\n",
    "    pickle.dump(save_Classifier, model_file)\n",
    "    model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "train_identity = pd.read_csv('./datasets/IEEEFraudDetection/train_identity.csv', index_col='TransactionID')\n",
    "train_transaction = pd.read_csv('./datasets/IEEEFraudDetection/train_transaction.csv', index_col='TransactionID')\n",
    "\n",
    "test_identity = pd.read_csv('./datasets/IEEEFraudDetection/test_identity.csv', index_col='TransactionID')\n",
    "test_transaction = pd.read_csv('./datasets/IEEEFraudDetection/test_transaction.csv', index_col='TransactionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transaction.head(), train_identity.head()\n",
    "# test_identity.head(), test_transaction.head()\n",
    "print(\"Shape of train_identity : \", train_identity.shape)\n",
    "print(\"Shape of train_transaction : \", train_transaction.shape)\n",
    "print(\"Shape of test_identity : \", test_identity.shape)\n",
    "print(\"Shape of test_transaction : \", test_transaction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = train_transaction.merge(train_identity,\n",
    "                                       how='left', left_index=True, right_index=True)\n",
    "test_merged = test_transaction.merge(test_identity,\n",
    "                                     how='left',left_index=True, right_index=True)\n",
    "\n",
    "print(\"Shape of train_merged_identity : \", train_merged.shape)\n",
    "print(\"Shape of train_merged_transaction : \", test_merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction columns (394 items including 'TransactionID')\n",
    "\n",
    "1. TransactionID, isFraud, TransactionDT, TransactionAmt, ProductCD,\n",
    "2. card1 - card6,\n",
    "3. addr1 - addr2,\n",
    "4. dist1 - dist2,\n",
    "5. P_emaildomain, R_emaildomain,\n",
    "6. C1 - C14,\n",
    "7. D1 - D15,\n",
    "8. M1 - M9,\n",
    "9. V1 - V339\n",
    "\n",
    "# Identity columns (41 items including 'TransactionID')\n",
    "1. TransactionID,\n",
    "2. id_01 - id_38,\n",
    "3. DeviceType, DeviceInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_merged[19900:19910]\n",
    "print(\"Merged DataFrame shape :\", train_merged.shape)\n",
    "train_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Features - Transaction\n",
    "1. ProductCD\n",
    "2. card1 - card6\n",
    "3. addr1, addr2\n",
    "4. P_emaildomain\n",
    "5. R_emaildomain\n",
    "6. M1 - M9\n",
    "\n",
    "# Categorical Features - Identity\n",
    "7. DeviceType\n",
    "8. DeviceInfo\n",
    "9. id_12 - id_38<BR>\n",
    "<BR>** The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp). **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B>1. ProductCD</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized variables\n",
    "total_counts = len(train_merged)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "fig.suptitle('Categorical Features'.upper(), y=1.0, fontsize=14)\n",
    "\n",
    "df_dType = pd.DataFrame({'ProductCD' : train_merged['ProductCD']})\n",
    "df_id12 = pd.DataFrame({'id_12' : train_merged['id_12']})\n",
    "#df_dType['ProductCD'] = df_dType['ProductCD'].fillna(-999)\n",
    "\n",
    "ProductCD = sns.countplot(x='ProductCD', data=df_dType, ax=axes[0])\n",
    "ProductCD.set_title('ProductCD', fontsize=14)\n",
    "for rectBox in ProductCD.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"ProductCD Counts :\", count)\n",
    "    \n",
    "    ProductCD.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "                   count + 1000,\n",
    "                   '{:1.2f}%'.format(count/total_counts*100),\n",
    "                   ha=\"center\", fontsize=11)\n",
    "\n",
    "#print(\"----------------------------------------\")\n",
    "Card4 = sns.countplot(x=train_merged['card4'], data=train_merged, ax=axes[1])\n",
    "Card4.set_title('card4', fontsize=14)\n",
    "for rectBox in Card4.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Card4 Counts :\", count)\n",
    "    \n",
    "    Card4.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "               count + 1000,\n",
    "               '{:1.2f}%'.format(count/total_counts*100),\n",
    "               ha=\"center\", fontsize=11)\n",
    "\n",
    "# Eg. To change the x-labels\n",
    "# plt.xticks(np.arange(3), (\"Missing\", \"Yes\", \"No\"))\n",
    "plt.show()\n",
    "\n",
    "print(\"Missing Data for ProductCD :\" + '{:1.2f}%'.format(train_merged['ProductCD'].isnull().sum() / total_counts * 100))\n",
    "print(\"Missing Data for card 3 :\" + '{:1.2f}%'.format(train_merged['card3'].isnull().sum() / total_counts * 100))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 8))\n",
    "fig.subplots_adjust(hspace=1.0)\n",
    "fig.suptitle('Categorical Feature counts'.upper(), y=1.02, fontsize=14)\n",
    "\n",
    "isFraud = sns.countplot(x=train_merged['isFraud'], data=train_merged, ax=axes[0][0])\n",
    "ProductCD = sns.countplot(x=train_merged['ProductCD'], data=train_merged, ax=axes[0][1])\n",
    "DeviceType = sns.countplot(x=train_merged['DeviceType'], data=train_merged, ax=axes[1][0])\n",
    "\n",
    "card4 = sns.countplot(x=train_merged['card4'], data=train_merged,ax=axes[1][1]).set_title(\"Card 4 by count\")\n",
    "card6 = sns.countplot(x=train_merged['card6'], data=train_merged, ax=axes[2][0]).set_title(\"Card 6 by count\")\n",
    "m1 = sns.countplot(x=train_merged['M1'], data=train_merged, ax=axes[2][1]).set_title(\"M1 by count\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B>2. card4 and card6</B>\n",
    "(Missing Data : 0.27%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_cols = [c for c in train_merged if c[0] == 'c']\n",
    "\n",
    "# Example : train_merged['id_01'].value_counts()\n",
    "for ctr in range(len(c_cols)):\n",
    "    col_name = c_cols[ctr]\n",
    "    df_unique = train_merged[col_name].nunique()\n",
    "    df_unique_counts = train_merged[col_name].value_counts()\n",
    "    print (\"No. of unique values in\", col_name, \" :\", df_unique)\n",
    "    #print (\"No. of counts per unique value in\", col_name, \" :\\n\", df_unique_counts)\n",
    "    print(\"Missing Data : \" + '{:1.2f}%'.format(train_merged[col_name].isnull().sum() / total_counts * 100))\n",
    "    \n",
    "train_merged[c_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "fig.suptitle('Categorical Features'.upper(), y=1.0, fontsize=14)\n",
    "df_card4 = pd.DataFrame({'card4' : train_merged['card4']})\n",
    "df_card6 = pd.DataFrame({'card4' : train_merged['card6']})\n",
    "\n",
    "#df_dType['ProductCD'] = df_dType['ProductCD'].fillna(-999)\n",
    "\n",
    "Card4 = sns.countplot(x=train_merged['card4'], data=train_merged, ax=axes[0])\n",
    "Card4.set_title('card4', fontsize=14)\n",
    "for rectBox in Card4.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Card4 Counts :\", count)\n",
    "    \n",
    "    Card4.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "               count + 1000,\n",
    "               '{:1.2f}%'.format(count/total_counts*100),\n",
    "               ha=\"center\", fontsize=11)\n",
    "\n",
    "#print(\"----------------------------------------\")\n",
    "Card6 = sns.countplot(x=train_merged['card6'], data=train_merged, ax=axes[1])\n",
    "Card6.set_title('card6', fontsize=14)\n",
    "for rectBox in Card6.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Card6 Counts :\", count)\n",
    "    \n",
    "    Card6.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "               count + 1000,\n",
    "               '{:1.2f}%'.format(count/total_counts*100),\n",
    "               ha=\"center\", fontsize=11)\n",
    "\n",
    "# Eg. To change the x-labels\n",
    "# plt.xticks(np.arange(3), (\"Missing\", \"Yes\", \"No\"))\n",
    "plt.show()\n",
    "\n",
    "print(\"Missing Data for card 4 :\" + '{:1.2f}%'.format(train_merged['card4'].isnull().sum() / total_counts * 100))\n",
    "print(\"Missing Data for card 6 :\" + '{:1.2f}%'.format(train_merged['card6'].isnull().sum() / total_counts * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card1_Grp = pd.DataFrame()\n",
    "card1_Grp['card1Count'] = train_merged.groupby(['card1'])['card1'].count()\n",
    "card1_Grp['card1'] = card1_Grp.index\n",
    "\n",
    "# There are too many Devices, so we will subset the top 20\n",
    "card1_grp_top = card1_Grp.sort_values(by='card1Count',ascending=False).head(20)\n",
    "order_card1 = card1_grp_top.sort_values(by='card1Count',ascending=False)['card1']\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale = 1.3)\n",
    "ax = sns.barplot(x='card1', y='card1Count', data=card1_grp_top, order=order_card1)\n",
    "ax.set_title('Top 20 ranking of card1', fontsize=18)\n",
    "for rectBox in ax.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Counts :\", count)    \n",
    "    ax.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "            count + 100,\n",
    "            '{:1.2f}%'.format(count/total_counts*100),\n",
    "            ha=\"center\", fontsize=20)\n",
    "\n",
    "\n",
    "xt = plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "\n",
    "print(\"Total no. of card1 :\", len(card1_Grp.index))\n",
    "print(\"Missing Data for card1 : \" +\n",
    "      '{:1.2f}%'.format(train_merged['card1'].isnull().sum() / total_counts * 100))\n",
    "print(\"Top 20 :\\n\", card1_grp_top['card1'])\n",
    "\n",
    "for ctr in range(len(card1_grp_top)):\n",
    "    if (card1_grp_top.iloc[ctr]['card1'] == -999):\n",
    "        print(\"Found in Index\", str(ctr) + \":\", card1_grp_top.iloc[ctr]['card1Count'])\n",
    "        print(\"Missing Data : \" + '{:1.2f}%'.format(card1_grp_top.iloc[ctr]['card1Count'] / total_counts * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card2_Grp = pd.DataFrame()\n",
    "card2_Grp['card2Count'] = train_merged.groupby(['card2'])['card2'].count()\n",
    "card2_Grp['card2'] = card2_Grp.index\n",
    "\n",
    "# There are too many Devices, so we will subset the top 20\n",
    "card2_grp_top = card2_Grp.sort_values(by='card2Count',ascending=False).head(20)\n",
    "order_card2 = card2_grp_top.sort_values(by='card2Count',ascending=False)['card2']\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale = 1.3)\n",
    "ax = sns.barplot(x='card2', y='card2Count', data=card2_grp_top, order=order_card2)\n",
    "ax.set_title('Top 20 ranking of card2', fontsize=18)\n",
    "for rectBox in ax.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Counts :\", count)    \n",
    "    ax.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "            count + 1000,\n",
    "            '{:1.2f}%'.format(count/total_counts*100),\n",
    "            ha=\"center\", fontsize=20)\n",
    "\n",
    "\n",
    "xt = plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "\n",
    "print(\"Total no. of card2 :\", len(card2_Grp.index))\n",
    "print(\"Missing Data for card2 : \" +\n",
    "      '{:1.2f}%'.format(train_merged['card2'].isnull().sum() / total_counts * 100))\n",
    "print(\"Top 20 :\\n\", card2_grp_top['card2'])\n",
    "\n",
    "for ctr in range(len(card2_grp_top)):\n",
    "    if (card2_grp_top.iloc[ctr]['card2'] == -999):\n",
    "        print(\"Found in Index\", str(ctr) + \":\", card2_grp_top.iloc[ctr]['card2Count'])\n",
    "        print(\"Missing Data : \" + '{:1.2f}%'.format(card2_grp_top.iloc[ctr]['card2Count'] / total_counts * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card3_Grp = pd.DataFrame()\n",
    "card3_Grp['card3Count'] = train_merged.groupby(['card3'])['card3'].count()\n",
    "card3_Grp['card3'] = card3_Grp.index\n",
    "\n",
    "# There are too many Devices, so we will subset the top 20\n",
    "card3_grp_top = card3_Grp.sort_values(by='card3Count',ascending=False).head(20)\n",
    "order_card3 = card3_grp_top.sort_values(by='card3Count',ascending=False)['card3']\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale = 1.3)\n",
    "ax = sns.barplot(x='card3', y='card3Count', data=card3_grp_top, order=order_card3)\n",
    "ax.set_title('Top 20 ranking of card3', fontsize=18)\n",
    "for rectBox in ax.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Counts :\", count)    \n",
    "    ax.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "            count + 1000,\n",
    "            '{:1.2f}%'.format(count/total_counts*100),\n",
    "            ha=\"center\", fontsize=20)\n",
    "\n",
    "\n",
    "xt = plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "\n",
    "print(\"Total no. of card3 :\", len(card3_Grp.index))\n",
    "print(\"Missing Data for card3 : \" +\n",
    "      '{:1.2f}%'.format(train_merged['card3'].isnull().sum() / total_counts * 100))\n",
    "print(\"Top 20 :\\n\", card3_grp_top['card3'])\n",
    "\n",
    "for ctr in range(len(card3_grp_top)):\n",
    "    if (card3_grp_top.iloc[ctr]['card3'] == -999):\n",
    "        print(\"Found in Index\", str(ctr) + \":\", card3_grp_top.iloc[ctr]['card3Count'])\n",
    "        print(\"Missing Data : \" + '{:1.2f}%'.format(card3_grp_top.iloc[ctr]['card3Count'] / total_counts * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card5_Grp = pd.DataFrame()\n",
    "card5_Grp['card5Count'] = train_merged.groupby(['card5'])['card5'].count()\n",
    "card5_Grp['card5'] = card5_Grp.index\n",
    "\n",
    "# There are too many Devices, so we will subset the top 20\n",
    "card5_grp_top = card5_Grp.sort_values(by='card5Count',ascending=False).head(20)\n",
    "order_card5 = card5_grp_top.sort_values(by='card5Count',ascending=False)['card5']\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale = 1.3)\n",
    "ax = sns.barplot(x='card5', y='card5Count', data=card5_grp_top, order=order_card5)\n",
    "ax.set_title('Top 20 ranking of card5', fontsize=18)\n",
    "for rectBox in ax.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Counts :\", count)    \n",
    "    ax.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "            count + 1000,\n",
    "            '{:1.2f}%'.format(count/total_counts*100),\n",
    "            ha=\"center\", fontsize=20)\n",
    "\n",
    "xt = plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "\n",
    "print(\"Total no. of card5 :\", len(card5_Grp.index))\n",
    "print(\"Missing Data for card5 : \" +\n",
    "      '{:1.2f}%'.format(train_merged['card5'].isnull().sum() / total_counts * 100))\n",
    "print(\"Top 20 :\\n\", card5_grp_top['card5'])\n",
    "\n",
    "for ctr in range(len(card5_grp_top)):\n",
    "    if (card5_grp_top.iloc[ctr]['card5'] == -999):\n",
    "        print(\"Found in Index\", str(ctr) + \":\", card5_grp_top.iloc[ctr]['card5Count'])\n",
    "        print(\"Missing Data : \" + '{:1.2f}%'.format(card5_grp_top.iloc[ctr]['card5Count'] / total_counts * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B>3. addr1 and addr2</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dataframe with 2 cols: addr2 and addr2 count\n",
    "addr1_Grp = pd.DataFrame()\n",
    "addr1_Grp['addr1Count'] = train_merged.groupby(['addr1'])['addr1'].count()\n",
    "addr1_Grp['addr1'] = addr1_Grp.index\n",
    "\n",
    "# There are too many Devices, so we will subset the top 20\n",
    "addr1_grp_top = addr1_Grp.sort_values(by='addr1Count',ascending=False).head(20)\n",
    "order_addr1 = addr1_grp_top.sort_values(by='addr1Count',ascending=False)['addr1']\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale = 1.3)\n",
    "ax = sns.barplot(x='addr1', y='addr1Count', data=addr1_grp_top, order=order_addr1)\n",
    "ax.set_title('Top 20 ranking of addr1', fontsize=18)\n",
    "for rectBox in ax.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Counts :\", count)    \n",
    "    ax.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "            count + 100,\n",
    "            '{:1.2f}%'.format(count/total_counts*100),\n",
    "            ha=\"center\", fontsize=20)\n",
    "\n",
    "xt = plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "\n",
    "print(\"Total no. of addr1 :\", len(addr1_Grp.index))\n",
    "print(\"Missing Data for addr1 : \" +\n",
    "      '{:1.2f}%'.format(train_merged['addr1'].isnull().sum() / total_counts * 100))\n",
    "print(\"Top 20 :\\n\", addr1_grp_top['addr1'])\n",
    "\n",
    "for ctr in range(len(addr1_grp_top)):\n",
    "    if (addr1_grp_top.iloc[ctr]['addr1'] == -999):\n",
    "        print(\"Found in Index\", str(ctr) + \":\", addr1_grp_top.iloc[ctr]['addr1Count'])\n",
    "        print(\"Missing Data : \" + '{:1.2f}%'.format(addr1_grp_top.iloc[ctr]['addr1Count'] / total_counts * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_merged['addr1'], index = train_merged.index)\n",
    "# Getting the unqiue values\n",
    "df = df.nunique() #.value_counts()\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dataframe with 2 cols: addr2 and addr2 count\n",
    "addr2_Grp = pd.DataFrame()\n",
    "addr2_Grp['addr2Count'] = train_merged.groupby(['addr2'])['addr2'].count()\n",
    "addr2_Grp['addr2'] = addr2_Grp.index\n",
    "\n",
    "# There are too many Devices, so we will subset the top 20\n",
    "addr2_grp_top = addr2_Grp.sort_values(by='addr2Count',ascending=False).head(20)\n",
    "order_addr2 = addr2_grp_top.sort_values(by='addr2Count',ascending=False)['addr2']\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale = 1.3)\n",
    "ax = sns.barplot(x='addr2', y='addr2Count', data=addr2_grp_top, order=order_addr2)\n",
    "ax.set_title('Top 20 ranking of addr2', fontsize=18)\n",
    "for rectBox in ax.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Counts :\", count)    \n",
    "    ax.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "            count + 1000,\n",
    "            '{:1.2f}%'.format(count/total_counts*100),\n",
    "            ha=\"center\", fontsize=20)\n",
    "\n",
    "\n",
    "xt = plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "\n",
    "print(\"Total no. of addr2 :\", len(addr2_Grp.index))\n",
    "print(\"Missing Data for addr2 : \" +\n",
    "      '{:1.2f}%'.format(train_merged['addr2'].isnull().sum() / total_counts * 100))\n",
    "print(\"Top 20 :\\n\", addr2_grp_top['addr2'])\n",
    "\n",
    "\n",
    "for ctr in range(len(addr2_grp_top)):\n",
    "    if (addr2_grp_top.iloc[ctr]['addr2'] == -999):\n",
    "        print(\"Found in Index\", str(ctr) + \":\", addr2_grp_top.iloc[ctr]['addr2Count'])\n",
    "        print(\"Missing Data : \" + '{:1.2f}%'.format(addr2_grp_top.iloc[ctr]['addr2Count'] / total_counts * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_merged['addr2'], index = train_merged.index)\n",
    "# Getting the unqiue values\n",
    "df = df.nunique() #.value_counts()\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B>4. P_emaildomain</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dataframe with 2 cols: P_emaildomain and Email count\n",
    "P_EmailGrp = pd.DataFrame()\n",
    "P_EmailGrp['P_emailCount'] = train_merged.groupby(['P_emaildomain'])['P_emaildomain'].count()\n",
    "P_EmailGrp['P_emaildomain'] = P_EmailGrp.index\n",
    "\n",
    "# There are too many P_emaildomain, we will just take the top 10\n",
    "group_top = P_EmailGrp.sort_values(by='P_emailCount',ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale = 1.3)\n",
    "ax = sns.barplot(x=\"P_emaildomain\", y=\"P_emailCount\", data=group_top)\n",
    "ax.set_title('P_emaildomain', fontsize=18)\n",
    "for rectBox in ax.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Counts :\", count)    \n",
    "    ax.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "            count + 1000,\n",
    "            '{:1.2f}%'.format(count/total_counts*100),\n",
    "            ha=\"center\", fontsize=20)\n",
    "\n",
    "\n",
    "xt = plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "\n",
    "print(\"Total no. of P_emaildomain :\", len(P_EmailGrp.index))\n",
    "print(\"Missing Data for P_emaildomain : \" +\n",
    "      '{:1.2f}%'.format(train_merged['P_emaildomain'].isnull().sum() / total_counts * 100))\n",
    "\n",
    "for ctr in range(len(group_top)):\n",
    "    if (group_top.iloc[ctr]['P_emaildomain'] == -999):\n",
    "        print(\"Found in Index\", str(ctr) + \":\", group_top.iloc[ctr]['P_emailCount'])\n",
    "        print(\"Missing Data : \" + '{:1.2f}%'.format(group_top.iloc[ctr]['P_emailCount'] / total_counts * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B>5. R_emaildomain</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dataframe with 2 cols: R_emaildomain and Email count\n",
    "R_EmailGrp = pd.DataFrame()\n",
    "R_EmailGrp['R_emailCount'] = train_merged.groupby(['R_emaildomain'])['R_emaildomain'].count()\n",
    "R_EmailGrp['R_emaildomain'] = R_EmailGrp.index\n",
    "\n",
    "# There are too many Devices, so we will subset the top 10\n",
    "group_top = R_EmailGrp.sort_values(by='R_emailCount',ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale = 1.3)\n",
    "ax = sns.barplot(x=\"R_emaildomain\", y=\"R_emailCount\", data=group_top)\n",
    "ax.set_title('R_emaildomain', fontsize=18)\n",
    "for rectBox in ax.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Counts :\", count)    \n",
    "    ax.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "            count + 1000,\n",
    "            '{:1.2f}%'.format(count/total_counts*100),\n",
    "            ha=\"center\", fontsize=20)\n",
    "\n",
    "\n",
    "xt = plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "\n",
    "print(\"Total no. of R_emaildomain :\", len(R_EmailGrp.index))\n",
    "print(\"Missing Data for R_emaildomain : \" +\n",
    "      '{:1.2f}%'.format(train_merged['R_emaildomain'].isnull().sum() / total_counts * 100))\n",
    "\n",
    "for ctr in range(len(group_top)):\n",
    "    if (group_top.iloc[ctr]['R_emaildomain'] == -999):\n",
    "        print(\"Found in Index\", str(ctr) + \":\", group_top.iloc[ctr]['R_emailCount'])\n",
    "        print(\"Missing Data : \" + '{:1.2f}%'.format(group_top.iloc[ctr]['R_emailCount'] / total_counts * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B>6. M1 - M9</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1_loc = train_merged.columns.get_loc(\"M1\")\n",
    "M9_loc = train_merged.columns.get_loc(\"M9\")\n",
    "df_m = train_merged.iloc[:,M1_loc:M9_loc+1] #subset dataframe M1-M9\n",
    "df_m['isFraud'] = train_merged.isFraud \n",
    "\n",
    "df_m_cols = df_m.columns\n",
    "figure, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "count = 0\n",
    "for i in range(3): # rows loop\n",
    "    for j in range(3): # cols loop\n",
    "        mplot = sns.countplot(x=df_m_cols[count], hue = 'isFraud', data=df_m, ax=axes[i,j])\n",
    "        count += 1 # to loop over col-names\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_m_cols.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ctr in range(len(df_m_cols.values)): # df_m_cols loop\n",
    "    col_name = df_m_cols[ctr]\n",
    "    df_m1_unique = df_m[col_name].nunique()\n",
    "    df_m1_unique_counts = df_m[col_name].value_counts()\n",
    "    print (\"No. of unique values in\", col_name, \" :\", df_m1_unique)\n",
    "    print (\"No. of counts per unique value in \", col_name, \" :\\n\", df_m1_unique_counts)\n",
    "    print(\"Missing Data for\", col_name + \" : \" +\n",
    "          '{:1.2f}%'.format(train_merged[col_name].isnull().sum() / total_counts * 100))\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B>7. DeviceType</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dType = pd.DataFrame({'DeviceType' : train_merged['DeviceType']})\n",
    "df_dType['DeviceType'] = df_dType['DeviceType'].fillna(-999)\n",
    "\n",
    "ax = sns.countplot(x='DeviceType', data=df_dType)\n",
    "ax.set_title('DeviceType', fontsize=14)\n",
    "for rectBox in ax.patches:\n",
    "    count = rectBox.get_height()\n",
    "    print(\"Counts :\", count)\n",
    "    \n",
    "    ax.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "            count + 1000,\n",
    "            '{:1.2f}%'.format(count/total_counts*100),\n",
    "            ha=\"center\", fontsize=11)\n",
    "\n",
    "# Change the x-labels\n",
    "#plt.xticks(np.arange(3), (\"Missing\", \"Yes\", \"No\"))\n",
    "plt.show()\n",
    "\n",
    "print(\"Missing Data : \" + '{:1.2f}%'.format(train_merged['DeviceType'].isnull().sum() / total_counts * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B>8. DeviceInfo</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dataframe with 2 cols: device info and the count by device\n",
    "D_InfoGrp = pd.DataFrame()\n",
    "D_InfoGrp['DeviceCount'] = train_merged.groupby(['DeviceInfo'])['DeviceInfo'].count()\n",
    "D_InfoGrp['DeviceInfo'] = D_InfoGrp.index\n",
    "\n",
    "# There are too many Devices, so we will subset the top 20\n",
    "group_top = D_InfoGrp.sort_values(by='DeviceCount',ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale = 1.3)\n",
    "ax = sns.barplot(x=\"DeviceInfo\", y=\"DeviceCount\", data=group_top)\n",
    "ax.set_title('DeviceInfo', fontsize=18)\n",
    "\n",
    "for rectBox in ax.patches:\n",
    "    count = rectBox.get_height()\n",
    "    #print(\"Counts :\", count)    \n",
    "    ax.text(rectBox.get_x() + rectBox.get_width()/2.,\n",
    "            count + 1000,\n",
    "            '{:1.2f}%'.format(count/total_counts*100),\n",
    "            ha=\"center\", fontsize=20)\n",
    "\n",
    "\n",
    "xt = plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "\n",
    "print(\"Total no. of DeviceInfo :\", len(D_InfoGrp.index))\n",
    "print(\"Missing Data for DeviceInfo : \" +\n",
    "      '{:1.2f}%'.format(train_merged['DeviceInfo'].isnull().sum() / total_counts * 100))\n",
    "\n",
    "for ctr in range(len(group_top)):\n",
    "    if (group_top.iloc[ctr]['DeviceInfo'] == -999):\n",
    "        print(\"Found in Index\", str(ctr) + \":\", group_top.iloc[ctr]['DeviceCount'])\n",
    "        print(\"Missing Data : \" + '{:1.2f}%'.format(group_top.iloc[ctr]['DeviceCount'] / total_counts * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B>9. Id_01 - 38</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized variables\n",
    "total_counts = len(train_merged)\n",
    "\n",
    "i_cols = [c for c in train_merged if c[0] == 'i']\n",
    "\n",
    "# Example : train_merged['id_01'].value_counts()\n",
    "for ctr in range(len(i_cols)):\n",
    "    col_name = i_cols[ctr]\n",
    "    df_unique = train_merged[col_name].nunique()\n",
    "    df_unique_counts = train_merged[col_name].value_counts()\n",
    "    print (\"No. of unique values in\", col_name, \" :\", df_unique)\n",
    "    #print (\"No. of counts per unique value in\", col_name, \" :\\n\", df_unique_counts)\n",
    "    print(\"Missing Data : \" + '{:1.2f}%'.format(train_merged[col_name].isnull().sum() / total_counts * 100))\n",
    "    \n",
    "train_merged[i_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_pc = ((train_merged.isnull().sum() / total_counts) * 100).round(2).astype(str) + \"%\"\n",
    "missing_data_pc.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all values in data\n",
    "sorted_missing_data_pc = missing_data_pc.sort_values(ascending=False)\n",
    "\n",
    "print(\"Missing values in data :\\n\" + sorted_missing_data_pc.to_string())\n",
    "print(\"\\nTotal number of columns :\", len(sorted_missing_data_pc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_features = len(sorted_missing_data_pc)\n",
    "selected_num_features = 0\n",
    "for ctr in range(total_num_features): # cols loop\n",
    "    float_val = float(sorted_missing_data_pc[ctr].rstrip('%'))\n",
    "    if float_val > 77.00 :\n",
    "        selected_num_features += 1\n",
    "        print(\"'\" + sorted_missing_data_pc.index[ctr] + \"',\") #, \" :\", correlation_fraud[ctr])\n",
    "\n",
    "print(\"Total selected features :\", selected_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(16, 10))\n",
    "fig.subplots_adjust(hspace=1.0)\n",
    "fig.suptitle('Categorical Feature counts'.upper(), y=1.02, fontsize=14)\n",
    "\n",
    "isFraud = sns.countplot(x=train_merged['isFraud'], data=train_merged, ax=axes[0][0])\n",
    "ProductCD = sns.countplot(x=train_merged['ProductCD'], data=train_merged, ax=axes[0][1])\n",
    "DeviceType = sns.countplot(x=train_merged['DeviceType'], data=train_merged, ax=axes[1][0])\n",
    "card4 = sns.countplot(x=train_merged['card4'], data=train_merged, ax=axes[1][1]).set_title(\"Card 4 by count\")\n",
    "card6 = sns.countplot(x=train_merged['card6'], data=train_merged, ax=axes[2][0]).set_title(\"Card 6 by count\")\n",
    "m1 = sns.countplot(x=train_merged['M1'], data=train_merged, ax=axes[2][1]).set_title(\"M1 by count\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Parameters is Fraud instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset fraud dataset\n",
    "addr = 'addr1'\n",
    "addrC = 'addr1Count'\n",
    "fraud = pd.DataFrame()\n",
    "is_fraud = train_merged[train_merged['isFraud']==1]\n",
    "\n",
    "fraud[addrC] = is_fraud.groupby([addr])[addr].count()\n",
    "fraud[addr] = fraud.index\n",
    "\n",
    "# Subset NOT fraud dataset\n",
    "NOfraud = pd.DataFrame()\n",
    "no_fraud = train_merged[train_merged['isFraud']==0]\n",
    "NOfraud[addrC] = no_fraud.groupby([addr])[addr].count()\n",
    "NOfraud[addr] = NOfraud.index\n",
    "\n",
    "# There are too many addr, so we will subset the top 20\n",
    "group_top_f = fraud.sort_values(by=addrC,ascending=False).head(20)\n",
    "order_f = group_top_f.sort_values(by=addrC,ascending=False)[addr]\n",
    "\n",
    "group_top_l = NOfraud.sort_values(by=addrC,ascending=False).head(20)\n",
    "order_l = group_top_l.sort_values(by=addrC,ascending=False)[addr]\n",
    "\n",
    "f, axes = plt.subplots(4, 1, figsize=(18, 20))\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale = 1.3)\n",
    "ax = sns.barplot(x=addr, y=addrC, data=group_top_f, order = order_f, ax=axes[0])\n",
    "bx = sns.barplot(x=addr, y=addrC, data=group_top_l, order = order_l, ax=axes[1])\n",
    "\n",
    "az = sns.barplot(x=addr, y=addrC, data=group_top_f, ax=axes[2])\n",
    "bz = sns.barplot(x=addr, y=addrC, data=group_top_l, ax=axes[3])\n",
    "\n",
    "font_size= {'size': 'x-large'}\n",
    "ax.set_title(\"Fraud transactions by addr1 (ranked)\", **font_size)\n",
    "bx.set_title(\"Legit transactions by addr1 (ranked)\", **font_size)\n",
    "\n",
    "az.set_title(\"Fraud transactions by addr1\", **font_size)\n",
    "bz.set_title(\"Legit transactions by addr1\", **font_size)\n",
    "\n",
    "xt = plt.xticks(rotation=60)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of Frauds :\", is_fraud.shape)\n",
    "print(\"No. of Non-Frauds :\", no_fraud.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fraud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_merged.columns\n",
    "\n",
    "print(\"Train Features : \", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "sns.set(color_codes=True)\n",
    "card4 = sns.countplot(x='card4', hue=\"isFraud\", data=train_merged, ax=axes[0])\n",
    "card6 = sns.countplot(x='card6', hue=\"isFraud\", data=train_merged, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 \n",
    "order_p=train_merged.P_emaildomain.value_counts().iloc[:10].index\n",
    "order_r=train_merged.R_emaildomain.value_counts().iloc[:10].index\n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "p_email = sns.countplot(y='P_emaildomain',  hue=\"isFraud\", data=train_merged, order = order_p, ax=axes[0])\n",
    "r_email = sns.countplot(y='R_emaildomain',  hue=\"isFraud\", data=train_merged, order = order_r, ax=axes[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting columns that starts with 'C'\n",
    "c_cols = [c for c in train_merged if c[0] == 'C']\n",
    "train_merged[c_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_merged[c_cols], index = train_merged.index)\n",
    "\n",
    "#df.C1.unique()\n",
    "#output = df.drop_duplicates()\n",
    "#output.groupby('C1').size()\n",
    "\n",
    "# Getting the unqiue values\n",
    "df = df.nunique() #.value_counts()\n",
    "\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cols = [c for c in train_merged if c[0] == 'D']\n",
    "train_merged[d_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cols = [c for c in train_merged if c[0] == 'M']\n",
    "train_merged[m_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cols = [c for c in train_merged if c[0] == 'V']\n",
    "train_merged[v_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_cols = [c for c in train_merged if c[0] == 'T']\n",
    "train_merged[T_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign values to NaN in the column\n",
    "train_merged['R_emaildomain'] = train_merged['R_emaildomain'].fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_merged['isFraud'].copy()\n",
    "\n",
    "# Drop target, fill in NaNs\n",
    "X_train = train_merged.drop('isFraud', axis=1)\n",
    "X_test = test_merged.copy()\n",
    "\n",
    "# Eg.'DeviceType' type 'O' has been changed from \"int64\" to \"object\" by fillna\n",
    "X_train = X_train.fillna(-999)\n",
    "X_test = X_test.fillna(-999)\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "y = y_train.value_counts().values\n",
    "sns.barplot(y_train.value_counts().index, y)\n",
    "\n",
    "plt.title('Ground Truth count')\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape does not include \"TranscationID\", because it is used for index_col\n",
    "X_train.shape, X_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[0:10]['card4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine different Machine Learning models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = Pipeline([\n",
    "    (\"standard_scaler\", StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete data to free up memory \n",
    "del train_merged, test_merged, train_transaction, train_identity\n",
    "del test_transaction, test_identity\n",
    "\n",
    "# Garbage Collection\n",
    "gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_merged.dtypes\n",
    "num_obj_types = 0\n",
    "for column_name in X_train.columns:\n",
    "    if X_train[column_name].dtype=='object':\n",
    "        num_obj_types += 1\n",
    "        print(column_name)\n",
    "        \n",
    "print(\"No of object types to be encoded :\", num_obj_types, \"out of\", len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding before model training\n",
    "for index in X_train.columns:\n",
    "    if X_train[index].dtype=='object' or X_test[index].dtype=='object': \n",
    "        # print(X_train[index])\n",
    "        # Encode labels with value between 0 and n_classes-1\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(X_train[index].values) + list(X_test[index].values))\n",
    "        X_train[index] = lbl.transform(list(X_train[index].values))\n",
    "        X_test[index] = lbl.transform(list(X_test[index].values))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoded and transformed\n",
    "X_train.iloc[0:10]['card4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "#X_train = std_scaler.fit_transform(X_train)\n",
    "#X_test = std_scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  If XGBClassifier model exists, load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "retrieve_xgbc_file = './datasets/IEEEFraudDetection/XGBClassifier_model_pickle'\n",
    "\n",
    "if os.path.exists(retrieve_xgbc_file):\n",
    "    #xgb_Classifier.save_model and load_model give an \"le\" error when trying to obtain score\n",
    "    # Unpickling saved binary file if exist so that training do not need to done\n",
    "    loaded_XGBClassifier = load_classifier_from_picklefile(retrieve_xgbc_file)\n",
    "    print(\"Unpickling existing XGBClassifier model...\")\n",
    "    print(\"Loaded Classifier :\\n\", loaded_XGBClassifier)\n",
    "    print(\"with type\\n\", type(loaded_XGBClassifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_XGBClassifier.score(X_train, y_train)\n",
    "\n",
    "# 0.9897060317675348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Cross Validation to check the performance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "loaded_XGBClassifier_auc_scores = cross_val_score(loaded_XGBClassifier, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "loaded_XGBClassifier_auc_scores, loaded_XGBClassifier_auc_scores.mean()\n",
    "\n",
    "# (array([0.85256255, 0.74023   , 0.80202556]), 0.798272703167005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictions = loaded_XGBClassifier.predict_proba(X_test)\n",
    "create_file_for_submission(\"./datasets/IEEEFraudDetection/loaded_simple_xgboost.csv\", xgb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach \n",
    "1. drop the following 51 columns gives better scoring\n",
    "2. fill the missing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = ['TransactionDT',\n",
    "            'V300', 'V309', 'V111', 'C3', 'V124', 'V106', 'V125', 'V315', 'V134', 'V102',\n",
    "            'V123', 'V316', 'V113', 'V136', 'V305', 'V110', 'V299', 'V289', 'V286', 'V318',\n",
    "            'V103', 'V304', 'V116', 'V298', 'V284', 'V293', 'V137', 'V295', 'V301', 'V104',\n",
    "            'V311', 'V115', 'V109', 'V119', 'V321', 'V114', 'V133', 'V122', 'V319', 'V105',\n",
    "            'V112', 'V118', 'V117', 'V121', 'V108', 'V135', 'V320', 'V303', 'V297', 'V120',\n",
    "            'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "#drop_col = ['TransactionDT']\n",
    "\n",
    "X_train.drop(drop_col,axis=1, inplace=True)\n",
    "X_test.drop(drop_col, axis=1, inplace=True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_col = ['TransactionDT',\n",
    "#            'V300', 'V309', 'V111', 'C3', 'V124', 'V106', 'V125', 'V315', 'V134', 'V102',\n",
    "#            'V123', 'V316', 'V113', 'V136', 'V305', 'V110', 'V299', 'V289', 'V286', 'V318',\n",
    "#            'V103', 'V304', 'V116', 'V298', 'V284', 'V293', 'V137', 'V295', 'V301', 'V104',\n",
    "#            'V311', 'V115', 'V109', 'V119', 'V321', 'V114', 'V133', 'V122', 'V319', 'V105',\n",
    "#            'V112', 'V118', 'V117', 'V121', 'V108', 'V135', 'V320', 'V303', 'V297', 'V120']\n",
    "\n",
    "# drop 123 columns with missing data more than 77%\n",
    "drop_col = [\n",
    "'id_24',\n",
    "'id_26',\n",
    "'id_21',\n",
    "'id_07',\n",
    "'id_25',\n",
    "'id_08',\n",
    "'id_23',\n",
    "'id_27',\n",
    "'id_22',\n",
    "'dist2',\n",
    "'D7',\n",
    "'id_18',\n",
    "'D13',\n",
    "'D14',\n",
    "'D12',\n",
    "'id_04',\n",
    "'id_03',\n",
    "'D6',\n",
    "'id_33',\n",
    "'D8',\n",
    "'D9',\n",
    "'id_10',\n",
    "'id_09',\n",
    "'id_30',\n",
    "'id_32',\n",
    "'id_34',\n",
    "'id_14',\n",
    "'V151',\n",
    "'V152',\n",
    "'V153',\n",
    "'V154',\n",
    "'V155',\n",
    "'V156',\n",
    "'V164',\n",
    "'V157',\n",
    "'V158',\n",
    "'V159',\n",
    "'V160',\n",
    "'V161',\n",
    "'V162',\n",
    "'V149',\n",
    "'V150',\n",
    "'V163',\n",
    "'V165',\n",
    "'V138',\n",
    "'V139',\n",
    "'V140',\n",
    "'V141',\n",
    "'V142',\n",
    "'V143',\n",
    "'V144',\n",
    "'V145',\n",
    "'V146',\n",
    "'V166',\n",
    "'V147',\n",
    "'V148',\n",
    "'V338',\n",
    "'V339',\n",
    "'V336',\n",
    "'V335',\n",
    "'V322',\n",
    "'V323',\n",
    "'V324',\n",
    "'V325',\n",
    "'V326',\n",
    "'V327',\n",
    "'V328',\n",
    "'V329',\n",
    "'V330',\n",
    "'V331',\n",
    "'V332',\n",
    "'V333',\n",
    "'V334',\n",
    "'V337',\n",
    "'DeviceInfo',\n",
    "'id_13',\n",
    "'id_16',\n",
    "'V254',\n",
    "'V244',\n",
    "'V246',\n",
    "'V247',\n",
    "'V248',\n",
    "'V249',\n",
    "'V252',\n",
    "'V253',\n",
    "'V257',\n",
    "'V242',\n",
    "'V258',\n",
    "'V260',\n",
    "'V261',\n",
    "'V219',\n",
    "'V263',\n",
    "'V264',\n",
    "'V243',\n",
    "'V241',\n",
    "'V266',\n",
    "'V229',\n",
    "'V217',\n",
    "'V223',\n",
    "'V224',\n",
    "'V225',\n",
    "'V226',\n",
    "'V228',\n",
    "'V230',\n",
    "'V240',\n",
    "'V231',\n",
    "'V232',\n",
    "'V233',\n",
    "'V235',\n",
    "'V236',\n",
    "'V237',\n",
    "'V265',\n",
    "'V262',\n",
    "'V267',\n",
    "'V274',\n",
    "'V268',\n",
    "'V278',\n",
    "'V277',\n",
    "'V276',\n",
    "'V275',\n",
    "'V218',\n",
    "'V273',\n",
    "'V269'\n",
    "]\n",
    "\n",
    "# Replace NaN with \"gmail.com\" in 'R_emaildomain'\n",
    "X_train.drop(drop_col,axis=1, inplace=True)\n",
    "X_test.drop(drop_col, axis=1, inplace=True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers and Predictions\n",
    "## (1) XGradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Else create XGBClassifier model and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_Classifier = xgb.XGBClassifier(n_estimators=500, n_jobs=4, max_depth=9, learning_rate=0.05,\n",
    "                                   subsample=0.9, colsample_bytree=0.9, missing=-999, \n",
    "                                   gamma = 0.1, alpha = 4)\n",
    "\n",
    "xgb_Classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the mean accuracy on the given test data and labels\n",
    "xgb_Classifier.score(X_train, y_train)\n",
    "\n",
    "# 0.9897669929217326 for full dataset\n",
    "# 0.9893317980153757 for dropping 51 columns\n",
    "# 0.9891607681105429 for dropping 'TransactionDT'\n",
    "# 0.9888322552240323 for dropping columns with missing data of more than 75%\n",
    "# 0.989768686287127 for dropping columns with missing data of more than 77%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictions = xgb_Classifier.predict_proba(X_test)\n",
    "# xgb_predictions.shape gives (506691, 2)\n",
    "# xgb_predictions[0:2, 1] gives 2 values in the second column\n",
    "\n",
    "# Pickling files\n",
    "print(\"Pickling XGBClassifier model...\")\n",
    "filename = './datasets/IEEEFraudDetection/XGBClassifier_model_pickle'\n",
    "save_classifier_to_picklefile(filename, xgb_Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_for_submission(\"./datasets/IEEEFraudDetection/XGBClassifier_partial_dataset.csv\", xgb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the best combination of hyperparameter values\n",
    "# ** This will take more than 5+ hours **\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# loaded_XGBClassifier.get_params().keys()\n",
    "kfold=3\n",
    "\n",
    "colsample_bytrees = [0.9, 0.95]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "learning_rates = [0.05, 0.10]\n",
    "max_depths = [9, 10]\n",
    "missings = [-999]\n",
    "n_estimators = [100, 200, 500]\n",
    "subsamples = [0.9, 0.95]\n",
    "\n",
    "xgb_Classifier_param_grid = {#'colsample_bytree': colsample_bytrees,\n",
    "                             'gamma': gammas,\n",
    "                             #'learning_rate': learning_rates,\n",
    "                             #'max_depth': max_depths,\n",
    "                             #'missing': missings,\n",
    "                             'n_estimators': n_estimators,\n",
    "                             #'subsample': subsamples\n",
    "                            }\n",
    "\n",
    "# Tuning the param for GridSearch and performance has been increased to approx. x% \n",
    "grid_search = GridSearchCV(xgb_Classifier, xgb_Classifier_param_grid,\n",
    "                           cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) XGradient Boost Random Forest Classifier\n",
    "\n",
    "###  If XGBRFClassifier model exists, load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_xgbrfc_file = './datasets/IEEEFraudDetection/XGBRFClassifier_model_pickle'\n",
    "\n",
    "if os.path.exists(retrieve_xgbrfc_file):\n",
    "    # Unpickling saved binary file if exist so that training do not need to done\n",
    "    loaded_XGBRFClassifier = load_classifier_from_picklefile(retrieve_xgbrfc_file)\n",
    "    print(\"Unpickling existing XGBClassifier model...\")\n",
    "    print(\"Loaded Classifier :\\n\", loaded_XGBRFClassifier)\n",
    "    print(\"with type\\n\", type(loaded_XGBRFClassifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_XGBRFClassifier.score(X_train, y_train)\n",
    "\n",
    "# 0.9757188336099164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrf_predictions = loaded_XGBRFClassifier.predict_proba(X_test)\n",
    "create_file_for_submission(\"./datasets/IEEEFraudDetection/loaded_simple_xgbrf.csv\", xgbrf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Else create XGBRFClassifier model and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_RFClassifier = xgb.XGBRFClassifier(n_estimators=500,\n",
    "                        n_jobs=4,\n",
    "                        max_depth=9,\n",
    "                        learning_rate=0.05,\n",
    "                        subsample=0.9,\n",
    "                        colsample_bytree=0.9,\n",
    "                        missing=-999)\n",
    "\n",
    "# scale_pos_weight=1\n",
    "xgb_RFClassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_RFClassifier.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrf_predictions = xgb_RFClassifier.predict_proba(X_test)\n",
    "\n",
    "# Pickling files\n",
    "print(\"Pickling XGBRandomForestClassifier model...\")\n",
    "filename = './datasets/IEEEFraudDetection/XGBRFClassifier_model_pickle'\n",
    "save_classifier_to_picklefile(filename, xgb_RFClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_for_submission(\"./datasets/IEEEFraudDetection/XGBRFClassifier.csv\", xgbrf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_Classifier = BaggingClassifier(DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "                                   max_samples=50000, bootstrap=True, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_Classifier.fit(X_train, y_train)\n",
    "bag_predictions = bag_Classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, bag_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_Classifier.score(X_train, y_train)\n",
    "\n",
    "# 0.9781183323737596 partial dataset\n",
    "# 0.9780556778541674 full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_predictions = np.reshape(bag_predictions, (bag_predictions.shape[0], 1))\n",
    "classifier_index = np.reshape(X_test.index, (X_test.index.shape[0], 1))\n",
    "\n",
    "print(\"The reshape of Prediction numpy array : \", classifier_predictions.shape)\n",
    "classifier_predicted_results = np.concatenate((classifier_index, classifier_predictions), axis=1)\n",
    "\n",
    "classifier_predicted_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_for_submission(\"./datasets/IEEEFraudDetection/Bag_Classifier_partial_dataset.csv\", classifier_predicted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) SVM Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_classifier = LinearSVC(class_weight='balanced')\n",
    "svm_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Cross Validation to check the performance of Support Vector Machine\n",
    "svm_scores = cross_val_score(svm_classifier, X_train, y_train, cv=3)\n",
    "svm_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "svm_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_predictions = np.reshape(svm_predictions, (svm_predictions.shape[0], 1))\n",
    "classifier_index = np.reshape(X_test.index, (X_test.index.shape[0], 1))\n",
    "\n",
    "print(\"The reshape of Prediction numpy array : \", classifier_predictions.shape)\n",
    "classifier_predicted_results = np.concatenate((classifier_index, classifier_predictions), axis=1)\n",
    "\n",
    "classifier_predicted_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_for_submission(\"./datasets/IEEEFraudDetection/svm_classifier.csv\", classifier_predicted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning : Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "retrieve_sftvc_file = './datasets/IEEEFraudDetection/SoftVotingClassifier_model_pickle'\n",
    "\n",
    "if os.path.exists(retrieve_sftvc_file):\n",
    "    #xgb_Classifier.save_model and load_model give an \"le\" error when trying to obtain score\n",
    "    # Unpickling saved binary file if exist so that training do not need to done\n",
    "    loaded_SoftVoteClassifier = load_classifier_from_picklefile(retrieve_sftvc_file)\n",
    "    print(\"Unpickling existing SoftVoteClassifier model...\")\n",
    "    print(\"Loaded Classifier :\\n\", loaded_SoftVoteClassifier)\n",
    "    print(\"with type\\n\", type(loaded_SoftVoteClassifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_SoftVoteClassifier.score(X_train, y_train)\n",
    "\n",
    "# 0.9894046127273343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sftvc_predictions = loaded_SoftVoteClassifier.predict_proba(X_test)\n",
    "create_file_for_submission(\"./datasets/IEEEFraudDetection/loaded_simple_sftvc.csv\", sftvc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[('XGB_classifier', loaded_XGBClassifier),\n",
    "                ('XGB_RF_classifier', loaded_XGBRFClassifier),],\n",
    "    n_jobs=4,\n",
    "    voting='soft') \n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "voting_classifier.get_params() # gives parameters of the VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_classifier.score(X_train, y_train)\n",
    "\n",
    "# 0.9894046127273343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling files\n",
    "print(\"Pickling VotingClassifier model...\")\n",
    "filename = './datasets/IEEEFraudDetection/SoftVotingClassifier_model_pickle'\n",
    "save_classifier_to_picklefile(filename, voting_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_classifier_auc_scores = cross_val_score(voting_classifier, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "voting_classifier_auc_scores, voting_classifier_auc_scores.mean()\n",
    "\n",
    "# (array([0.87149802, 0.74444081, 0.78814395]), 0.8013609304639683)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probabilities of possible outcomes for samples\n",
    "voting_classifier_predictions = voting_classifier.predict_proba(X_test)\n",
    "\n",
    "voting_classifier_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample of Predictions\")\n",
    "print(\"SoftVoting :\\n\", voting_classifier_predictions[10:20, 1])\n",
    "print(\"XGBRF : \\n\", xgbrf_predictions[10:20, 1])\n",
    "print(\"XGB : \\n\", xgb_predictions[10:20, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_for_submission(\"./datasets/IEEEFraudDetection/Soft_Voting_Classifier.csv\", voting_classifier_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (\"Predicted {} frauds\".format(int([voting_classifier_predictions['isFraud']==1].sum())))\n",
    "print(\"Predicted {} frauds\".format(int((voting_classifier_predictions[:, 1]>0.5).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the best combination of hyperparameter values\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# loaded_XGBClassifier.get_params().keys()\n",
    "kfold=3\n",
    "\n",
    "colsample_bytrees = [0.9, 0.95]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "learning_rates = [0.05, 0.10]\n",
    "max_depths = [9, 10]\n",
    "missings = [0, -999]\n",
    "n_estimators = [100, 200]\n",
    "subsamples = [0.9, 0.95]\n",
    "\n",
    "voting_classifier_param_grid = {'colsample_bytree': colsample_bytrees,\n",
    "                                'gamma': gammas,\n",
    "                                'learning_rate': learning_rates,\n",
    "                                'max_depth': max_depths,\n",
    "                                'missing': missings,\n",
    "                                'n_estimators': n_estimators,\n",
    "                                'subsample': subsamples\n",
    "                               }\n",
    "\n",
    "# Tuning the param for GridSearch and performance has been increased to approx. x% \n",
    "grid_search = GridSearchCV(voting_classifier, voting_classifier_param_grid,\n",
    "                           cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions\n",
    "\n",
    "- kaggle competitions submit -c ieee-fraud-detection -f submission.csv -m \"Message\"\n",
    "- Submitted Public score of 0.9366 at position 321 with XGBClassifier with mean score of 98.971%\n",
    "- Submitted Public score of 0.8543 with scaled features for XGBClassifier with mean score of 98.964%\n",
    "- Submitted Public score of 0.8757 with XGBRFClassifier having mean score of 97.572%\n",
    "- Submitted Public score of 0.5240 with balanced weight SVM Linear Classifier with mean cv score of 96.437%.\n",
    "- Submitted Public score of 0.9375 at position 334 using Soft Voting Classifier with XGBC and XGBRFC gives a mean score of 98.940%.\n",
    "- Submitted Public score of <B>0.9376 at position 343</B> using XGBClassifier with partial dataset and score of 98.933%.\n",
    "<BR>(Suspect over fitting with skewed dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.intersect1d(pd.Series([1,2,3,5,42]), pd.Series([4,5,6,20,42])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['ProductCD',\n",
    "            'card1', 'card2', 'card3', 'card4','card5', 'card6',\n",
    "            'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain',\n",
    "            'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "            'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21',\n",
    "            'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31',\n",
    "            'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
    "            'DeviceType', 'DeviceInfo']\n",
    "\n",
    "len(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx = 0\n",
    "value = 4685.0\n",
    "found_pos_idx = 0\n",
    "\n",
    "for c in X_train['C1']:\n",
    "    row_idx = row_idx + 1\n",
    "    if c == value:\n",
    "        found_pos_idx = row_idx\n",
    "        #print(\"Found\", value)\n",
    "\n",
    "print(found_pos_idx, \"of\", row_idx)\n",
    "\n",
    "c=X_train['C1']\n",
    "c[found_pos_idx-1:found_pos_idx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if value exists in pandas data frame\n",
    "if getattr(X_train, 'C1').isin(['4685.0']).any():\n",
    "    print(\"Found 4685.0\")\n",
    "\n",
    "c = X_train['C1']\n",
    "c[0:10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_merged.dtypes\n",
    "# Before LabelEncoder\n",
    "X_train['DeviceType'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = preprocessing.LabelEncoder()\n",
    "lbl.fit(list(X_train['DeviceType'].values) + list(X_test['DeviceType'].values))\n",
    "X_train['DeviceType'] = lbl.transform(list(X_train['DeviceType'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After LabelEncoder\n",
    "X_train['DeviceType'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "corr_matrix = train_merged.corr()\n",
    "correlation_fraud = corr_matrix['isFraud'].sort_values(ascending=False)\n",
    "\n",
    "total_num_features = len(correlation_fraud)\n",
    "selected_num_features = 0\n",
    "for ctr in range(total_num_features): # cols loop\n",
    "    if correlation_fraud[ctr] > -0.04 and correlation_fraud[ctr] < 0.04:\n",
    "        selected_num_features += 1\n",
    "        print(\"'\" + correlation_fraud.index[ctr] + \"',\") #, \" :\", correlation_fraud[ctr])\n",
    "\n",
    "print(\"Total selected features :\", selected_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ctr = list(range(12, 39)) # Range from 12 - 38\n",
    "id_list = []\n",
    "for num in id_ctr:\n",
    "    numStr = \"id_\" + str(num) \n",
    "    id_list.append(numStr)\n",
    "\n",
    "print(id_list)\n",
    "\n",
    "for identifier in id_list:\n",
    "    df = pd.DataFrame(train_merged[identifier], index = train_merged.index) # TransactionID is the index\n",
    "    # Getting the unqiue values\n",
    "    df = df.nunique()\n",
    "    #print (df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
