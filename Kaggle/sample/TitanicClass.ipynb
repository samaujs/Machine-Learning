{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloaded data from https://www.kaggle.com/c/titanic/data\n",
    "# \"kaggle competitions download -c titanic\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TITANIC_PATH = os.path.join(\"datasets\", \"titanic\")\n",
    "\n",
    "def load_titanic_data(filename, titanic_path=TITANIC_PATH):\n",
    "    csv_path = os.path.join(titanic_path, filename)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# Create submission file with assigned predicted results from models\n",
    "def create_file_for_submission(filename, classifier_predictions):\n",
    "    \n",
    "    classifier_predictions = np.reshape(classifier_predictions, (classifier_predictions.shape[0], 1))\n",
    "    print(\"The reshape of Prediction numpy array : \", classifier_predictions.shape)\n",
    "\n",
    "    classifier_predicted_results = np.concatenate((passengerID, classifier_predictions), axis=1)\n",
    "    print(\"The concatenation of PassengerId and Prediction numpy arrays  : \", classifier_predicted_results.shape)\n",
    "\n",
    "    print(\"For PassengerIds : \", classifier_predicted_results[0:10,0])\n",
    "    print(\"The respective predicted results : \", classifier_predicted_results[0:10,1])\n",
    "    \n",
    "    # Create and overwrite existing file\n",
    "    with open('datasets/titanic/' + filename, 'w') as writeFile:\n",
    "        filewriter = csv.writer(writeFile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(['PassengerId', 'Survived'])\n",
    "\n",
    "        iteration_range = classifier_predicted_results.shape[0]\n",
    "        for i in range(iteration_range):\n",
    "            filewriter.writerow([str(classifier_predicted_results[i,0]), str(classifier_predicted_results[i,1])])\n",
    "        #np.savetxt(writeFile, model_predicted_results[0:10], delimiter=\",\") gives decimals\n",
    "\n",
    "    writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the respective Train and Test data\n",
    "train_data = load_titanic_data(\"train.csv\")\n",
    "test_data = load_titanic_data(\"test.csv\")\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.swarmplot(x=train_data['Survived'], y=train_data['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a new column with initial value '0', avoid the \"SettingWithCopyWarning\" when setting the values to int\n",
    "test_data['Survived'] = 0\n",
    "test_data.head()\n",
    "\n",
    "# Merge train and test with append that creates float values for \"Survived\" column if values do not exist\n",
    "titanic_merged_data = train_data.append(test_data, ignore_index=True, sort=False)\n",
    "\n",
    "# create indexes to separate data later on\n",
    "train_data_idx = len(train_data) # 891\n",
    "test_data_idx = len(titanic_merged_data) - len(test_data) # 891\n",
    "\n",
    "# Create a new feature to extract title names from the Name column\n",
    "titanic_merged_data['Title'] = titanic_merged_data.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
    "\n",
    "# Show value counts for the different titles\n",
    "print(titanic_merged_data.Title.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the titles\n",
    "normalized_titles = {\n",
    "    \"Capt\":       \"Officer\",\n",
    "    \"Col\":        \"Officer\",\n",
    "    \"Major\":      \"Officer\",\n",
    "    \"Jonkheer\":   \"Royalty\",\n",
    "    \"Don\":        \"Royalty\",\n",
    "    \"Sir\" :       \"Royalty\",\n",
    "    \"Dr\":         \"Officer\",\n",
    "    \"Rev\":        \"Officer\",\n",
    "    \"the Countess\":\"Royalty\",\n",
    "    \"Dona\":       \"Royalty\",\n",
    "    \"Mme\":        \"Mrs\",\n",
    "    \"Mlle\":       \"Miss\",\n",
    "    \"Ms\":         \"Mrs\",\n",
    "    \"Mr\" :        \"Mr\",\n",
    "    \"Mrs\" :       \"Mrs\",\n",
    "    \"Miss\" :      \"Miss\",\n",
    "    \"Master\" :    \"Master\",\n",
    "    \"Lady\" :      \"Royalty\"\n",
    "}\n",
    "\n",
    "# Map the normalized titles to the current titles \n",
    "titanic_merged_data.Title = titanic_merged_data.Title.map(normalized_titles)\n",
    "\n",
    "# Show value counts for the normalized titles\n",
    "print(titanic_merged_data.Title.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Sex, Pclass, and Title to estimate missing Age\n",
    "grouped = titanic_merged_data.groupby(['Sex','Pclass', 'Title'])\n",
    "\n",
    "# Show the median Age by the grouped features \n",
    "grouped.Age.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a closer approximation of what a passengerâ€™s age might have been from the group by Sex, Pclass, and Title\n",
    "# Apply the grouped median value on the Age that has the value of NaN\n",
    "titanic_merged_data.Age = grouped.Age.apply(lambda age: age.fillna(age.median()))\n",
    "\n",
    "# Before applying Median Age in group\n",
    "# titanic_merged_data.Age.head(20)\n",
    "titanic_merged_data.Age.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Cabin NaN with \"U\" for unknown\n",
    "titanic_merged_data.Cabin = titanic_merged_data.Cabin.fillna('U')\n",
    "\n",
    "# Obtain the most frequent Embarked value and store in a variable, \"most_embarked\"\n",
    "most_embarked = titanic_merged_data.Embarked.value_counts().index[0]\n",
    "\n",
    "# Fill NaN with the value of \"most_embarked\"\n",
    "titanic_merged_data.Embarked = titanic_merged_data.Embarked.fillna(most_embarked)\n",
    "\n",
    "# Fill NaN with the median fare\n",
    "titanic_merged_data.Fare = titanic_merged_data.Fare.fillna(titanic_merged_data.Fare.median())\n",
    "\n",
    "# view changes\n",
    "titanic_merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Create a new feature for the size of families (including the passenger)\n",
    "titanic_merged_data['FamilySize'] = titanic_merged_data.Parch + titanic_merged_data.SibSp + 1\n",
    "\n",
    "#titanic_merged_data['AgeGroup'] = (titanic_merged_data.Age // 15) * 15 + 15 # Set the Age Group\n",
    "\n",
    "# Map first letter of cabin to itself (section where the room would have been)\n",
    "titanic_merged_data.Cabin = titanic_merged_data.Cabin.map(lambda cabin: cabin[0])\n",
    "\n",
    "titanic_merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the male and female groups to integer form\n",
    "titanic_merged_data.Sex = titanic_merged_data.Sex.map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "# Create dummy variables for categorical features\n",
    "pclass_dummies = pd.get_dummies(titanic_merged_data.Pclass, prefix=\"Pclass\")\n",
    "title_dummies = pd.get_dummies(titanic_merged_data.Title, prefix=\"Title\")\n",
    "cabin_dummies = pd.get_dummies(titanic_merged_data.Cabin, prefix=\"Cabin\")\n",
    "embarked_dummies = pd.get_dummies(titanic_merged_data.Embarked, prefix=\"Embarked\")\n",
    "\n",
    "# Concatenate dummy columns with main dataset\n",
    "titanic_merged_data_dummies = pd.concat([titanic_merged_data, pclass_dummies, title_dummies, cabin_dummies, embarked_dummies], axis=1)\n",
    "\n",
    "# Drop the categorical fields and unused \"Ticket\", \"Name\" features\n",
    "titanic_merged_data_dummies.drop(['Pclass', 'Title', 'Cabin', 'Embarked', 'Name', 'Ticket'], axis=1, inplace=True)\n",
    "\n",
    "titanic_merged_data_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_training_data = pd.DataFrame(titanic_merged_data, columns=\n",
    "                                     ['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'FamilySize'])\n",
    "\n",
    "partial_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(10,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(partial_training_data.astype(float).corr(),linewidths=0.1,vmax=1.0, \n",
    "            square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save PassengerId for final submission\n",
    "passengerId = test_data.PassengerId\n",
    "# Convert from DataFrames to Numpy Array\n",
    "passengerID = test_data['PassengerId'].values # get from DataFrame to Numpy Array\n",
    "passengerID = np.reshape(passengerID, (passengerID.shape[0], 1))\n",
    "\n",
    "# Create the train and test data for training and prediction\n",
    "X_train = titanic_merged_data_dummies[ :train_data_idx] # need to have leading space\n",
    "X_test = titanic_merged_data_dummies[test_data_idx: ]   # need to have trailing space\n",
    "\n",
    "# Convert \"Survived\" back to int\n",
    "#X_train.Survived = X_train.Survived.astype(int)\n",
    "\n",
    "X_train.head()\n",
    "# Create X and y for data and ground truth label values\n",
    "y_train = X_train.Survived.values\n",
    "X_train = X_train.drop('Survived', axis=1).values \n",
    "\n",
    "# create array for test set\n",
    "X_test = X_test.drop('Survived', axis=1).values\n",
    "\n",
    "X_train.shape, y_train.shape, passengerId.shape, passengerID.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifiers and Predictions\n",
    "- <B>XGradient Boost Random Forest Classifier</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "\n",
    "xgbrf_Classifier = xgb.XGBRFClassifier(n_estimators=500,\n",
    "                        n_jobs=4,\n",
    "                        max_depth=9,\n",
    "                        learning_rate=0.05,\n",
    "                        subsample=0.9,\n",
    "                        colsample_bytree=0.9,\n",
    "                        missing=-999)\n",
    "\n",
    "xgbrf_Classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the mean accuracy on the given test data and labels\n",
    "xgbrf_Classifier.score(X_train, y_train)\n",
    "\n",
    "# xgb_RFClassifier gives 0.9068462401795735\n",
    "# xgb_Classifier gives 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrf_predictions = xgbrf_Classifier.predict(X_test)\n",
    "\n",
    "# dataframe with predictions\n",
    "kaggle = pd.DataFrame({'PassengerId': passengerId, 'Survived': xgbrf_predictions})\n",
    "# save to csv\n",
    "kaggle.to_csv('./datasets/titanic/xgbrf_classifier.csv', index=False)\n",
    "\n",
    "#passengerId.shape, xgb_predictions.shape give ((418,), (418,))\n",
    "xgbrf_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrf_Classifier.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the best combination of hyperparameter values\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "kfold=3\n",
    "\n",
    "colsample_bytrees = [0.9, 0.95]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "learning_rates = [0.05, 0.10]\n",
    "max_depths = [9, 10]\n",
    "missings = [0, -999]\n",
    "n_estimators = [100, 200]\n",
    "subsamples = [0.9, 0.95]\n",
    "\n",
    "xbgrf_classifier_param_grid = {'colsample_bytree': colsample_bytrees, 'gamma': gammas,\n",
    "                               'learning_rate': learning_rates, 'max_depth': max_depths,\n",
    "                               'missing': missings, 'n_estimators': n_estimators,\n",
    "                               'subsample': subsamples}\n",
    "\n",
    "# Tuning the param for GridSearch and performance has been increased to approx. 83.056% \n",
    "grid_search = GridSearchCV(xgbrf_Classifier, xbgrf_classifier_param_grid,\n",
    "                           cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GridSearchCV to obtain final model with best estimator\n",
    "best_xgbrf_classifier_model = grid_search.best_estimator_\n",
    "best_xgbrf_classifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgbrf_classifier_model.score(X_train, y_train)\n",
    "\n",
    "# 0.9057239057239057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Cross Validation to check the performance of XGB Random Forest Classifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "best_xgbrf_classifier_scores = cross_val_score(best_xgbrf_classifier_model, X_train, y_train, cv=10)\n",
    "best_xgbrf_classifier_scores.mean()\n",
    "\n",
    "# 0.8305575417092271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgbrf_predictions = best_xgbrf_classifier_model.predict(X_test)\n",
    "best_xgbrf_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with predictions\n",
    "kaggle = pd.DataFrame({'PassengerId': passengerId, 'Survived': best_xgbrf_predictions})\n",
    "# save to csv\n",
    "kaggle.to_csv('./datasets/titanic/best_xgbrf_classifier.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin : Partial Test with RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create param grid object \n",
    "forrest_params = dict(     \n",
    "    max_depth = [n \n",
    "for n in range(9, 14)],     \n",
    "    min_samples_split = [n \n",
    "for n in range(4, 11)], \n",
    "    min_samples_leaf = [n \n",
    "for n in range(2, 5)],     \n",
    "    n_estimators = [n \n",
    "for n in range(10, 60, 10)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# instantiate Random Forest model\n",
    "forrest = RandomForestClassifier()\n",
    "\n",
    "# build and fit model \n",
    "forest_cv = GridSearchCV(estimator=forrest, param_grid=forrest_params, cv=10) \n",
    "forest_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best score: {}\".format(forest_cv.best_score_))\n",
    "print(\"Optimal params: {}\".format(forest_cv.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forrest prediction on test set\n",
    "forrest_pred = forest_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passengerId.shape, forrest_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with predictions\n",
    "kaggle = pd.DataFrame({'PassengerId': passengerId, 'Survived': forrest_pred})\n",
    "# save to csv\n",
    "kaggle.to_csv('./datasets/titanic/titanic_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End : Partial Test with RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number data\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "train_data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"Survived\", \"Pclass\", \"Fare\", \"Age\"]\n",
    "scatter_matrix(train_data[attributes], figsize=(18,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building preprocessing pipelines\n",
    "Selecting numerical and non-numerical attributes from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer to feed a Pandas DataFrame containing non-numerical columns directly into a pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self # do nothing\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# set the variables with respective index values of matrix X that starts from zero\n",
    "age_index, SibSp_index, Parch_index = 1, 2, 3\n",
    "\n",
    "# Value X is taken from param in class_instance.transform(param) \n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        return None # do nothing\n",
    "    def fit(self, X, y=None):\n",
    "        return self # do nothing\n",
    "    def transform(self, X, y=None):\n",
    "        Age_group = (X[:, age_index] // 15) * 15 # computed for all rows\n",
    "        Relatives_onboard = X[:, SibSp_index] + X[:, Parch_index]\n",
    "        # Added 2 attributes to X\n",
    "        return np.c_[X, Age_group, Relatives_onboard]\n",
    "\n",
    "# num_training_data.values[:,1] // 15\n",
    "attributes_adder = CombinedAttributesAdder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling with StandardScaler from Scikit-Learn\n",
    "# Transformation Numerical Pipeline and Compute median values to take care of the missing values in the training set\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# StandardScaler subtracts mean and divide by standard deviation giving unit variance\n",
    "selector_num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector([\"Age\", \"SibSp\", \"Parch\", \"Fare\"])),\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    #('attributes_adder', CombinedAttributesAdder()), # add attributes\n",
    "    ('std_scaler', StandardScaler()) # Scaling for better performance\n",
    "])\n",
    "\n",
    "# Fit the SimpleImputer instance into the training data using fit() method\n",
    "X = selector_num_pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age (float64), SibSp (int64), Parch (int64), Parch (int64)\n",
    "# X is a plain NumPy array containing the transformed features\n",
    "# Put X into a Pandas DataFrame\n",
    "num_training_data = pd.DataFrame(X, columns=[\"Age\", \"SibSp\", \"Parch\", \"Fare\"])\n",
    "#num_training_data = pd.DataFrame(X, columns=[\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Age_group\", \"Relatives_onboard\"])\n",
    "\n",
    "# Missing Age values are filled up with median values\n",
    "num_training_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.columns gives ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
    "#                          'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "# Fill NA/NaN values with most frequent values in each column\n",
    "\n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Panda Series Constructor that gives a One-dimensional ndarray with column names as axis labels\n",
    "        # X[col].value_counts().index[0] gives the element with highest value giving most frequent value\n",
    "        # For loop runs through all columns in the provided data, X\n",
    "        self.most_frequent_ = pd.Series([X[col].value_counts().index[0] for col in X],\n",
    "                                        index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.most_frequent_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Pclass (int64), Sex (object), Embarked (object)\n",
    "selector_category_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),\n",
    "    (\"imputer\", MostFrequentImputer()),\n",
    "    ('category_encoder', OneHotEncoder(categories='auto', sparse=False)),\n",
    "])\n",
    "\n",
    "print(\"Transforming with train data including non-numerical ...\")\n",
    "cleanedfeatures_train_category = selector_category_pipeline.fit_transform(train_data)\n",
    "\n",
    "# Display transformed categories\n",
    "cleanedfeatures_train_category[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing pipeline that takes the raw data and outputs numerical input features that can be fed to any Machine Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining pipelines with FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "complete_preprocess_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"number_pipeline\", selector_num_pipeline),\n",
    "    (\"category_pipeline\", selector_category_pipeline),\n",
    "])\n",
    "\n",
    "X_train = complete_preprocess_pipeline.fit_transform(train_data)\n",
    "\n",
    "print(\"The shape of prepared data features : \", X_train.shape)\n",
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the label from training data\n",
    "y_train = train_data[\"Survived\"]\n",
    "print(\"The shape of labels : \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from DataFrames to Numpy Array\n",
    "passengerID = test_data['PassengerId'].values # get from DataFrame to Numpy Array\n",
    "passengerID = np.reshape(passengerID, (passengerID.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Using Cross Validation to check the performance of Stochastic Gradient Descent\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# SGD with l2 regularization\n",
    "sgd_classifier = SGDClassifier(max_iter=5, tol=-np.infty, random_state=42, penalty='l2')\n",
    "sgd_classifier.fit(X_train, y_train)\n",
    "\n",
    "sgd_scores = cross_val_score(sgd_classifier, X_train, y_train, cv=10)\n",
    "sgd_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_reg = LogisticRegression(solver=\"liblinear\", random_state=42) # lbfgs\n",
    "logistic_reg.fit(X_train, y_train)\n",
    "\n",
    "logistic_reg_scores = cross_val_score(logistic_reg, X_train, y_train, cv=10)\n",
    "logistic_reg_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# svm_classifier = SVC(gamma=\"auto\")\n",
    "# gives mean score of 0.8260637271592328 gives highest ranking with new attributes\n",
    "# gives better mean score of 0.8283234025649756 but submission ranking is not as good\n",
    "# set hyperparameter, probability=True, adds predict_proba() method for soft voting classifier\n",
    "\n",
    "svm_prob_classifier = SVC(kernel=\"poly\", degree=2, coef0=1, C=5, gamma=\"auto\", probability=True) # degree=3\n",
    "svm_prob_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_prob_classifier.score(X_train, y_train)\n",
    "\n",
    "# 0.8282828282828283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Cross Validation to check the performance of Support Vector Machine\n",
    "svm_scores = cross_val_score(svm_prob_classifier, X_train, y_train, cv=3)\n",
    "svm_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm_prob_classifier.predict(X_test)\n",
    "svm_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with predictions\n",
    "kaggle = pd.DataFrame({'PassengerId': passengerId, 'Survived': svm_predictions})\n",
    "# save to csv\n",
    "kaggle.to_csv('./datasets/titanic/svm_classifier.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# [_NumericColumn(key='X', shape=(14,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n",
    "feature_cols = [tf.feature_column.numeric_column(\"X\", shape=[28])] # 12 or 14 features with 2 combined attributes\n",
    "dnn_classifier = tf.estimator.DNNClassifier(hidden_units=[300,100], n_classes=10,\n",
    "                                            feature_columns=feature_cols)\n",
    "\n",
    "input_function = tf.estimator.inputs.numpy_input_fn(x={\"X\": X_train},\n",
    "                                                    y=y_train, num_epochs=40, batch_size=50, shuffle=True)\n",
    "\n",
    "dnn_classifier.train(input_fn=input_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = dnn_classifier.evaluate(input_fn=input_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data is without the \"Survived\" labels\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the scaled test data set\n",
    "#X_test = complete_preprocess_pipeline.transform(test_data)\n",
    "\n",
    "test_input_function = tf.estimator.inputs.numpy_input_fn(x={\"X\": X_test}, shuffle=False)\n",
    "\n",
    "y_predicted_iterations = dnn_classifier.predict(input_fn=test_input_function)\n",
    "#evaluation_results = dnn_classifier.evaluate(input_fn=test_input_function)\n",
    "y_prediction = list(y_predicted_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_predictions_list = []\n",
    "\n",
    "for predicted_info in y_prediction:\n",
    "    dnn_predictions_list.append(predicted_info['class_ids'])\n",
    "\n",
    "y_dnn_predictions = np.asarray(dnn_predictions_list, dtype=int)\n",
    "\n",
    "y_dnn_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_for_submission(\"dnn_classifier.csv\", y_dnn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune hyperparameters of SVM Poly Classifier using cross validation and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Search the best combination of hyperparameter values\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "kfold=10\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "param_grid = {'C': Cs,\n",
    "              'gamma' : gammas}\n",
    "\n",
    "# kernel='rbf'\n",
    "# class_weight='balanced' will take more than 5 hours on t2 instance\n",
    "svm_classifier = SVC(param_grid, kernel=\"poly\", degree=3, coef0=1)\n",
    "\n",
    "# Tuning the param for GridSearch and performance has been increased to approx. 83.40% \n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4,\n",
    "                           verbose = 1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GridSearchCV to obtain final model with best estimator\n",
    "best_svm_classifier_model = grid_search.best_estimator_\n",
    "best_svm_classifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svm_classifier_scores = cross_val_score(best_svm_classifier_model, X_train, y_train, cv=10)\n",
    "best_svm_classifier_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_svm_predictions = best_svm_classifier_model.predict(X_test)\n",
    "\n",
    "y_svm_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_for_submission(\"svm_classifier.csv\", y_svm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune hyperparameters of Random Forest Classifier using cross validation and Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Learning : Using Random Forest Classifier for prediction\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest_scores = cross_val_score(forest_classifier, X_train, y_train, cv=10)\n",
    "forest_scores.mean()\n",
    "\n",
    "# 0.8205081716036773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold=10\n",
    "#rf_param_grid = {\"max_depth\": [None],\n",
    "#                 \"max_features\": ['auto'], # [1, 3, 10],\n",
    "#                 \"min_samples_split\": [2, 3, 10],\n",
    "#                 \"min_samples_leaf\": [1, 3, 10],\n",
    "#                 \"bootstrap\": [False],\n",
    "#                 \"n_estimators\" :[100, 200, 500],\n",
    "#                 \"criterion\": [\"gini\"]}\n",
    "\n",
    "rf_param_grid = {\"max_depth\": [n \n",
    "                    for n in range(9, 14)],\n",
    "                 \"max_features\": ['auto'], # [1, 3, 10],\n",
    "                 \"min_samples_split\": [n \n",
    "                    for n in range(4, 11)],\n",
    "                 \"min_samples_leaf\": [n \n",
    "                    for n in range(2, 11)],\n",
    "                 \"bootstrap\": [False],\n",
    "                 \"n_estimators\" :[n \n",
    "                    for n in range(100, 200, 500)],\n",
    "                 \"criterion\": [\"gini\"]}\n",
    "\n",
    "# Tuning the param for GridSearch and performance has been increased to approx. 83.40% \n",
    "grid_search = GridSearchCV(forest_classifier, rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GridSearchCV to obtain final model with best estimator\n",
    "best_random_forest_classifier_model = grid_search.best_estimator_\n",
    "best_random_forest_classifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_forest_classifier_model.score(X_train, y_train)\n",
    "\n",
    "# 0.8821548821548821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the Best Models and their Errors\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "print(\"Length of features\", len(feature_importances), \"with values :\")\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanic_merged_data_dummies.columns\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train,\n",
    "                          columns=['PassengerId', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
    "                                   'FamilySize', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Title_Master',\n",
    "                                   'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Officer', 'Title_Royalty',\n",
    "                                   'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F',\n",
    "                                   'Cabin_G', 'Cabin_T', 'Cabin_U', 'Embarked_C', 'Embarked_Q','Embarked_S'])\n",
    "\n",
    "X_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the importance scores with its corresponding attribute names\n",
    "# num_attributes = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "# extra_attributes = [\"Age_group\", \"Relatives_onboard\"]\n",
    "# category_one_hot_attributes = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "# all_attributes = num_attributes + category_one_hot_attributes + extra_attributes\n",
    "\n",
    "#train_data.columns gives ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
    "#                          'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "all_attributes = X_train_df.columns #train_data.columns\n",
    "\n",
    "sorted(zip(feature_importances, all_attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_feature_importances=pd.Series(feature_importances, index=all_attributes).sort_values(ascending=False)\n",
    "random_forest_feature_importances.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(x= random_forest_feature_importances.index[0:10], y= random_forest_feature_importances[0:10])\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_forest_classifier_scores = cross_val_score(best_random_forest_classifier_model, X_train, y_train, cv=10)\n",
    "best_random_forest_classifier_scores.mean()\n",
    "\n",
    "# 0.8384110770627625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test set with Random Forest Classifier\n",
    "y_randomforest_predictions = best_random_forest_classifier_model.predict(X_test)\n",
    "\n",
    "y_randomforest_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_for_submission(\"best_random_forest_classifier.csv\", y_randomforest_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boost_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "gradient_boost_classifier_scores = cross_val_score(gradient_boost_classifier, X_train, y_train, cv=10)\n",
    "gradient_boost_classifier_scores.mean()\n",
    "\n",
    "# 0.8115191238224947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boost_classifier.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "kfold=10\n",
    "gbc_param_grid = {\"max_depth\": [n \n",
    "                    for n in range(9, 14)],\n",
    "                 \"max_features\": ['auto'], # [1, 3, 10],\n",
    "                 \"min_samples_split\": [n \n",
    "                    for n in range(4, 11)],\n",
    "                 \"min_samples_leaf\": [n \n",
    "                    for n in range(2, 11)],\n",
    "                 \"n_estimators\" :[n \n",
    "                    for n in range(100, 200, 500)],\n",
    "                 \"criterion\": [\"friedman_mse\"]}\n",
    "\n",
    "grid_search = GridSearchCV(gradient_boost_classifier, gbc_param_grid, cv=kfold,\n",
    "                           scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gradient_boost_classifier_model.score(X_train, y_train)\n",
    "\n",
    "# 0.9966329966329966"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GridSearchCV to obtain final model with best estimator\n",
    "best_gradient_boost_classifier_model = grid_search.best_estimator_\n",
    "best_gradient_boost_classifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gbc_predictions = best_gradient_boost_classifier_model.predict(X_test)\n",
    "best_gbc_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with predictions\n",
    "kaggle = pd.DataFrame({'PassengerId': passengerId, 'Survived': best_gbc_predictions})\n",
    "# save to csv\n",
    "kaggle.to_csv('./datasets/titanic/best_gb_classifier.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning : Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#voting_classifier = VotingClassifier(\n",
    "#    estimators=[('svm_prob_classifier', svm_prob_classifier)\n",
    "#                ('logistic_regressor', logistic_reg),\n",
    "#                ('random_forest_classifier', best_random_forest_classifier_model)],\n",
    "#    voting='soft') \n",
    "\n",
    "#voting_classifier = VotingClassifier(\n",
    "#    estimators=[('best_svm_classifier', best_svm_classifier_model), # svm_prob_classifier\n",
    "                #('logistic_regressor', logistic_reg) or ('sgd_classifier', sgd_classifier),\n",
    "#                ('random_forest_classifier', best_random_forest_classifier_model)],\n",
    "#    voting='hard')\n",
    "\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[('xgbrf_Classifier', best_xgbrf_classifier_model), # xgbrf_Classifier\n",
    "                ('random_forest_classifier', best_random_forest_classifier_model)],\n",
    "    voting='hard') \n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "voting_classifier.get_params() # gives parameters of the VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_classifier_scores = cross_val_score(voting_classifier, X_train, y_train, cv=10)\n",
    "voting_classifier_scores.mean()\n",
    "\n",
    "# Hard XGBRFC and RF gives 0.8328050164567019\n",
    "# Hard XGBC and RF gives 0.822729542617183\n",
    "\n",
    "# Hard XGBRFC and RF gives 0.0.8316686528203382 : Need to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_classifier_predictions = voting_classifier.predict(X_test)\n",
    "\n",
    "voting_classifier_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_for_submission(\"voting_classifier.csv\", voting_classifier_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions\n",
    "\n",
    "- kaggle competitions submit -c titanic -f submission.csv -m \"Message\"\n",
    "- 83.503% gives ranking 2,244 of 11,567 (Top 20%)\n",
    "- 83.841% gives ranking 1,646 of 11,572 (Top 15%) with \"FamilySize\" that gives better scoring\n",
    "- 83.952% gives ranking 1,641 of 11,577 (Top 15%) with \"FamilySize\" and \"AgeGroup\" features\n",
    "\n",
    "# \"FamilySize\" feature\n",
    "- Public score of <B>0.80382 (Top 11%) at position 1,187</B> with Hard Voting Classifier (XGBRFC, RF GridSearchCV) and cross_val giving mean score of 83.281%\n",
    "- Public score of 0.77033 with XGB Classifier giving score of 80.815%.\n",
    "- Public score of 0.77990 with Hard Voting Classifier having best GridSearchCV XGB and Random Forest Classifiers that gives a mean cross validation mean score of 82.273%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot([1]*10, best_xgbrf_classifier_scores, \".\") # best_svm_classifier_scores\n",
    "plt.plot([2]*10, best_random_forest_classifier_scores, \".\")\n",
    "plt.boxplot([best_xgbrf_classifier_scores, best_random_forest_classifier_scores],\n",
    "            labels=(\"XGradient Boost\",\"Random Forest\")) # Support Vector Machine\n",
    "plt.ylabel(\"Accuracy\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe datasets to improve predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"AgeBucket\"] = (train_data[\"Age\"] // 15) * 15 + 15 # getting only the integer position and multiply by 15\n",
    "train_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"RelativesOnboard\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\n",
    "train_data[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Correlation Coefficient (Pearson's r)\n",
    "corrCoeff_matrix = train_data.corr()\n",
    "\n",
    "corrCoeff_matrix[\"Survived\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for understanding :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown of extracting Title\n",
    "Name = train_data.loc[4].Name\n",
    "\n",
    "Fullname = Name.split(',')[1]\n",
    "Title = Fullname.split('.')[0]\n",
    "Title_strip = Title.strip()\n",
    "\n",
    "# The strip() method returns a copy of the string with both leading and trailing characters removed\n",
    "# (based on the string argument passed).\n",
    "\n",
    "print(\"Fullname with alias :\" + Fullname)\n",
    "print(\"Title :\\\"\" + Title + \"\\\" with length of \" + str(len(Title)))\n",
    "print(\"Strip the leading and trailing chars :\\\"\" + Title_strip + \"\\\" with length of \" +str(len(Title_strip)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives the first element of each column from value_counts # \n",
    "# train_data[\"Embarked\"].value_counts()\n",
    "print(\"Most frequent used name : \" + str(train_data[\"Name\"].value_counts().index[0]) + \"\\n--\\n\")\n",
    "print(\"The complete list of most frequent used values in each column of the data :- \\n\")\n",
    "\n",
    "# train_data.shape is (891, 12)\n",
    "X = train_data\n",
    "for column in X:\n",
    "    print(str(column) + \" : \"+ str(X[column].value_counts().index[0]))\n",
    "\n",
    "most_frequent_values = pd.Series([train_data[\"PassengerId\"].value_counts().index[0],\n",
    "                                  train_data[\"Survived\"].value_counts().index[0],\n",
    "                                  train_data[\"Pclass\"].value_counts().index[0],\n",
    "                                  train_data[\"Name\"].value_counts().index[0],\n",
    "                                  train_data[\"Sex\"].value_counts().index[0]],\n",
    "                                 index=train_data.columns[:5])\n",
    "\n",
    "print(\"--\\n\\nThe partial list of most frequent used values in the created DataFrame :\")\n",
    "most_frequent_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
